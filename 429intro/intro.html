<!DOCTYPE html>
<html>  
  <head>
    <meta charset='utf-8' />
    <link rel="stylesheet" type="text/css" href="../stylesheets/stylesheet.css">
    <link rel="stylesheet" type="text/css" href="introstyle.css">
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
      	tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.1-latest/MathJax.js?config=TeX-AMS_HTML-full"></script>
	<script type="text/javascript" src="diagrams.js"></script>
    <title>Systems Intro</title>
  </head>
  <body onload="processDOM(document.body)">
    <div id="header_wrap" class="outer">
      <header class="inner">
        <a id="forkme_banner" href="https://github.com/fazzone/fazzone.github.com">View on GitHub</a>
        <h1 id="project_title">Introduction to "Computing Systems"</h1>
        <h4 id="project_tagline">"I am sorry to have written so long an introduction; I did not have time to write a short one"</h4>		
      </header>
    </div>
	
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
		<div id="contents" style="background-color: rgb(240,240,240);">
		  <h4>contents</h4>
		  (items that aren't links haven't been written yet)
		  <br>
		  (items with * next to them are actually interesting)
		  <ol>
			<li>
			  <a href="#overview">overview</a>
			  <ol>
				<li><a href="#meta">meta</a>
				  <ol>
					<li><a href="#whatisthis">what is this document?</a></li>
					<li><a href="#why">why did you write this?</a></li>
					<li><a href="#feedback">shameless solicitation for feedback</a></li>
				  </ol>
				</li>
				<li><a href="#preface">preface</a>
				  <ol>
					<li><a href="#computingsystems">what is meant by "Computing Systems"?</a></li>
					<li><a href="#whyinteresting">why is this interesting?</a></li>
					<li><a href="#myapproach">my approach</a></li>
				  </ol>
				</li>
			  </ol>
			</li>
			
			<li>
			  <a href="#data"><strong>Part I</strong> - Data & data storage</a>
			  <ol>
				<li><a href="#datavsinformation">* Data vs. Information</a></li>
				<li><a href="#datastorage">How computers store data</a></li>
				<li><a href="#integers">Representing integers:</a>
				  <ol>
					<li><a href="#bcd">Binary Coded Decimal</a></li>
					<li><a href="#onescomptwoscomp">One's Complement & Two's Complement</a></li>
				  </ol>
				</li>
				<li><a href="#text">Representing text:</a>
				  <ol>
					<li><a href="#ebcdic">EBCDIC</a></li>
					<li><a href="#ascii">ASCII</a></li>
					<li><a href="#unicode">Unicode</a></li>
				  </ol>
				</li>
				<li>Representing programs:
				  <ol>
					<li>Machine code</li>
					<li>Introduction to assembly language</li>
				  </ol>
				</li>
			  </ol>
			</li>
			<li>
			  <strong>Part II</strong> - Code & program execution
			  <ol>
				<li>CPUs & ISAs</li>
				<li>Accessing data:
				  <ol>
					<li>Registers</li>
					<li>Memory buses & addressing</li>
					<li>Harvard & von Neumann architectures</li>
					<li>Addressing modes</li>
				  </ol>
				</li>
				<li>Operating on data:
				  <ol>
					<li>Arithmetic</li>
					<li>Logical operations</li>
				  </ol>
				<li>Control:
				  <ol>
					<li>Jumps & Conditional jumps</li>
					<li>Loops in assembly</li>
					<li>Jump tables</li>
					<li>Procedures & the stack</li>
					<li>Interrupts</li>
				  </ol>
				</li>				
			  </ol>
			</li>

			<li><strong>Part III</strong> - Putting together the system
			  <ol>
				<li>OS Concepts
				  <ol>
					<li>Memory management & virtual memory</li>
					<li>Multitasking</li>
					<li>The system ABI/API</li>
					<li>The compiler & the linker</li>
				  </ol>
				</li>
				<li>Introduction to C (for Java programmers)
				  <ol>
					<li>The essence of C</li>
					<li><code>#include</code> and the preprocessor</li>
					<li>Pointers</li>					
					<li><code>struct</code> & <code>union</code></li>
					<li>Dynamic memory allocation: <code>malloc</code> & <code>free</code></li>
				  </ol>
				</li>
				<li>Counterpoint: Introduction to Lisp
				  <ol>
					<li>The essence of Lisp</li>
					<li>Symbols & S-expressions</li>
					<li>Cons cells</li>
					<li><code>eval</code></li>
					<li>Memory management in Lisp</li>
					<li>Bootstraping</li>
				  </ol>
				</li>
			  </ol>
			</li>

			<li><strong>Part IV</strong> - Making it Fast
			  <ol>
				<li>Caching</li>
				<li>Branch prediction</li>
				<li>Pipelining, superscalar architectures, and VLIW systems</li>
				<li>RISC vs CISC</li>
			  </ol>
			</li>
			
			
		  </ol>
		</div>
		
		<div id="overview">
		  <h2>overview - an introduction to the introduction</h2>
		  <div id="meta">
			<h3>meta</h3>
			<div id="whatisthis">
			  <h4>what is this document?</h4>
			  <p>This is intended to be a brief introduction to some lower-level problems & solutions in "computing systems" (more on this phrase and why it is
				used later)</p>
			  
			  <p>I assume a reasonable background in programming and general computer knowledge.</p>
			  
			  <p>You might find this page especially useful if you are (like me) a freshman Turing Scholar on winter break from the University of Texas at Austin, 
				who has completed CS 314H (Algorithms & Data Structures) and will take CS 429H (Computer Organization & Architecture) in the spring.</p>
			</div>
			<div id="why">
			  <h4>why did you write this?</h4>
			  <p>This is really a two-part question: 'Why were you motivated to write this?' and 'What qualifies you to write this?'.  The first one is easy:
				Break is really boring.  The second one is more difficult.  The most correct answer is probably 'nothing', but I think there are two reasons
				why you might want to read this as opposed to just breaking out the textbook.</p>
			  <ul>
				<li><strong>Specialization</strong> - I am writing with a very specific audience in mind, so I can reference and make assumptions about what you know.</li>
				<li><strong>Format</strong> - I am trying to be much more casual than a textbook, and much more brief.  In many places, I will probably trade absolute
				  completeness for brevity, though I will try not to sacrifice correctness.</li>
			  </ul>
			  <p>To sum up, the answer to the question "Why would I read this instead of a textbook?" is "Because this document has different design goals that I believe 
				make it more accessible and quicker to comprehend".</p>
			</div>
			<div id="feedback">
			  <h4>shameless solicitation for feedback</h4>
			  <p>This is (supposed to be) a living document.  Neither my knowledge nor my attention to detail are infinite (in fact both are much closer to infinitesimal). 
				Therefore, I'm probably wrong about a lot of stuff.  Because I'd always like to learn more, and because I'd like this document to be as good as it can be, 
				I entreat you to submit feedback!  If my writing is unclear, if I spell something wrong, if I'm wrong about something, or <i>especially</i> if you have an 
				unanswered question - please let me know!  This page lives on GitHub, so you can submit a pull request or
				<a href="https://github.com/fazzone/fazzone.github.com/issues">open an issue</a>.  Or, you can write me directly; me email address is rmcq@utexas.edu</p>
			</div>
		  </div>
		  <div id="preface">
			<h3>preface</h3>
			<div id="computingsystems">
			  <h4>what is meant by "Computing Systems"?</h4>
			  <p>
				A course on "Systems" is generally considered part of the core of an education in Computer Science, which is most generally defined as the study of
				computation.  In order to give that definition substance, we also have to define computation, which is rather difficult.  We humans have been computing
				since we became a species; our ancestors computed the optimum trajectory of their spears so that they could bring down a mammoth and eat, the best path
				to run to avoid being eaten by a tiger, and so on.  At some point, we invented numbers and other notation to make computation easier.  Now you could
				precisely compute the exact number of sheep you needed to shear to have enough wool to sell to buy food over the winter.  Since
				then, we've advanced slightly as a species, and our computing needs advanced with us.  Now we use machines to compute all sorts of things: the lowest 
				grade you can get on your final and still pass, the cheapest way to fly from Austin to New York, how to win a game of chess, and just about everything
				else.  Thus, the best definition I can think of for computation is 'transforming data in some meaningful way'.  You might also define a computation as:
				'a process which depends on some amount of information as input, and produces some amount of information as output'.  Of course, these definitions
				(just like our first) reference other terms; we will discuss technical definitions of the terms referenced in later sections (probably).
			  </p>
			  <p>The generality of my chosen definition of computation is intentional - but if we've been computing since the dawn of our species, why is the field of 
				computer science so young?  Digital computers and other fancy machines certainly make certain kinds of computation easier, but you certainly can study
				computation without them.  So why did computer science only really get established as a discipline until the invention of the computer?  To be honest,
				I don't have an answer.  The best I can come up with is that before we had machines to compute for us, the only way to compute something was to learn
				the necessary math (for computing is deeply entwined with mathematics) and work it out yourself.  The bottleneck of this process was comprehending the
				math - As long as you understood, mathematically, what you needed to compute, the computation itself was generally little more than an afterthought by
				comparison.  Only when we had to explain the process of computation to a machine, unable to 'understand' the math as we do, did we really think to 
				separate <i>what</i> was being computed with <i>how</i> it was computed.  And by that circuitous route we arrive at my definition of computer science:
				"the study of <i>how</i>"
			  <p>In accordance with its origins, the field of computer science is most often studied with digital computers - a computer program is essentially a 
				distillation of mathematical understanding into a form that can be fed to a "computing system".  This implies my definition of "Computing System":
				something that executes a program.  The computing systems that we are most interested in are 'stored-program digital computers' - this term reflects
				the fact that not all computers necessarily "store" their programs in a way that can be easily changed (it might be baked into the hardware), and that
				not all computers have to be digital (operating by electrical impulses that considered to be in a discrete set of states (commonly 1 and 0) - a compute
				might also be electromechanical (like <a href="https://en.wikipedia.org/wiki/Z3_(computer)">the Z-3</a>) or just mechanical (like Babbage's
				<a href="https://en.wikipedia.org/wiki/Analytical_engine">Analytical Engine</a>)).  The study of 'Systems', then, for our purposes, is the study of the
				properties, design, and implementation of stored-program digital computers.  
			  <p>
			</div>
			<div id="whyinteresting">
			  <h4>why is this interesting?</h4>
			  <p>A "Systems" class is a core part of pretty much every computer science degree because it gives you grounding in what is actually happening when you
				program.  I think this is interesting in and of itself, but it is also helpful when designing for performance - understanding what is going on at
				multiple levels of the system makes it easier to write performant programs.  Also, it is important to note that no programs execute in a vacuum - by
				definition, a program executes on a computer system, so it is good to have knowledge of the system(s) your program run(s) on.  A course in systems, 
				however, is about more than that.  As a programmer, you interact with sytems.  The point of education is to be a critical observer of the world around you
				- in order to be a critical observer of something, you must understand the <i>choices</i> in its design.  A student of political science analyzes 
				governments and can tell you why certain governments are they way they are (and the implications of this),  a student of literature analyzes different
				works and can tell you why certain works are written the way they are (and the implications of this), and a student of computer science should be able
				to do the same with programs.  Since a system defines the behavior of its programs, understanding systems is a necessary skill in computer science.				
			  </p>
			  <p>Furthermore, a good computer scientist should be educated in systems so as to be prepared for things going wrong 
				(and <a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">leaky abstractions</a>).  Abstraction is the key way we deal with complexity
				in computer science (and any field, really).  The goal of an abstraction is to 'seal off' complex implementation details and present a simple model for 
				understanding something.  "Pushing the gas makes the car go" is a good abstraction for your car's drivetrain - right up until something goes wrong.  Then
				all of a sudden your abstraction is broken and you have no idea why.  Similarly, you can just assume the entire stack of technology going from solid-state
				physics to the standard library of whatever programming language you are using "just works", or you can be prepared for things going wrong.  Most of the
				things in that open interval would be considered "systems".
			</div>
			<div id="myapproach">
			  <h4>my approach</h4>
			  <p>I couldn't help but skirt around it in previous sections, but my approach to talking about things is going to be to present problems & solutions.
				I think this is the cleanest way to quickly gain a deep understanding of all sorts of things that are designed by humans - all design efforts have
				problems, trade-offs, choices, pressures, goals, etc.  If you understand the problems, you can put yourself in the designers' shoes and think how
				you would come up with a solution - and then compare your way to the 'actual' way.  This approach is also easy to explain in chronological fashion -
				the first attempts at a design are often the simplest, and only after they are used and studied are their shortcomings realized and fixed in later
				designs.  In all cases where it is practical, I will try to present as many alternatives as possible to each design problem.  An easy way to check
				your understanding of what I'm talking about (or things in general) is to try to imagine the consequences if different choices were made in the design
				of some system.  Some examples: What would our government be like if we had <a href="https://en.wikipedia.org/wiki/Proportional_representation">
				  proportional representation</a> (as opposed to winner-takes-all elections) in the House & Senate?  What would our language be like if we used an 
				ideographic writing system (as does Chinese)?  These questions become more interesting the more fundamental the change: it is not very interesting to
				consider how Java would be different if it used a colon (like C++) instead of the word <code>extends</code> to denote inheritance - it would be more
				interesting to consider
				how Java would be different if it used <a href="https://en.wikipedia.org/wiki/Prototype-based_programming">prototypal inheritance</a> instead of the more 
				conservative class-based model.  I will try to include some such considerations as I discuss computing systems.
			  </p>
			</div>
		  </div>
		</div>
		
		<div id="data">
		  <h2>Data & data storage</h2>
		  <div id="datavsinformation">
			<h3>Data vs. Information</h3>
			<p>Before we get into a discussion of how different kinds of data are represented in different kinds of systems, I think that we should first discuss
			  the difference between data and information.  This might be a pretty fluffy distinction, and the concepts do certainly overlap, but I think that it's
			  worth discussing if only because I think the theory of information is very interesting.  To paraphrase <a href="https://en.wikipedia.org/wiki/Data">
				Wikipedia</a>, the difference between data and information is the level of abstraction.  Data is just, well data; the value of some variable.  
			  Only when a human interprets the data does it become information.  The fundamental unit of both is the bit - a binary distinction of some kind: "yes"
			  or "no", "true" or "false", "1" or "0", etc.  As an example (which I believe I am stealing from
			  <a href="http://www.amazon.com/Decoding-Universe-Information-Explaining-Everything/dp/0143038397"><i>Decoding the Universe</i></a>), consider the
			  lighting of the lanterns in the steeple of the Old North Church during the Revolutionary War - one if by land, two if by sea.  Anyone who saw the
			  steeple could have collected
			  the one bit of data (were there two lanterns or one?), but only the patriots across the river were able to decode this data into the corresponding one
			  bit of <i>information</i>: were the British coming by land or by sea?  This is the distinction between data & information at its most immaterial: here, both
			  are pretty much the same thing.  However, had the patriots already discovered by some other means that the British army was coming by sea, the one bit
			  of data would have revealed to them zero bits of information.  This suggests another distinction between information and data: information, by defintion,
			  is non-redundant.  If I write you a letter, but misspell some words, your ability to understand my meaning in spite of my poor spelling is an example of 
			  redundancy - I used more data than required to transmit the amount of information in the letter. </p>
			<p>The mathematical name for this concept is <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">information content</a> which, interestingly, 
			  is closely related to the notion of entropy in thermodynamics.  In information theory, the entropy of some variable is
			  a measure of how unpredictable (<i>non</i>-redundant) it is.  The previous example of the letter is made possible because "English text has about one bit of
			  entropy for each character"<sup><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_ref-8">[wiki]</a></sup>.  That statement implies
			  that, given a stream of characters representing English text, you could be expected to accurately guess the next character about half the time.  This is
			  how compression algorithms work - they exploit low-entropy source material and use less redundant forms of encoding to save (data) space.  Given a completely
			  random input, a compression algorithm should be expected to not compress it all - or possibly even make it larger (we see this by the pigeonhole principle; 
			  if a compression algorithm is able to make some inputs smaller, it must make a corresponding number of inputs larger - otherwise you'd be encoding information
			  "for free").  This, in turn, is why compressing something that is already compressed (a JPEG image or an MP3 music file) is stupid.  </p>
			<p>If you want to read more about information theory, the book I cited previously,
			  <a href="http://www.amazon.com/Decoding-Universe-Information-Explaining-Everything/dp/0143038397"><i>Decoding the Universe</i></a> is a great read.  You can
			  also read Claude Shannon's original paper <a href="https://docs.google.com/a/utexas.edu/viewer?a=v&q=cache:UlvwzqigMQIJ:cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf+&hl=en&gl=us&pid=bl&srcid=ADGEEShzLsrhAOZxjkAuOTPSBYERGoupPl-DgoyizF5Oqkxxwp-NXvR7e-Wo_-t5fqmLZZRBu-G4uRpREeOYdvdcLJAj9LYk6HwCYej-aGyiy5elzwPOpsg5SSZgVabP9mcrUus0Z8h1&sig=AHIEtbSz7BkLkwZD4sTDOmRe6GzVJAdAsg">A Mathematical Theory of Communication</a> - it is reasonably accessible, as far as papers which create entire new fields of mathematics go.  (Shannon is also famous for writing what is commonly hailed as the most important master's thesis of all time, in which he realized the connection between Boolean logic and
			  digital circuits - a direct antecedent to the construction of the very first digital computers).
		  </div>
		  
		  <div id="datastorage">
			<h3>How computers store data</h3>
			<p>As I discussed previously, the main focus of this modern computing is on digital stored-program computers.  Digital, in this term, refers to how these
			  computers store data.  In a digital system, we deal with discrete values (as opposed to an analog system, where we would deal with continuous values).
			  In the real world (quantum-mechanical nitpicks aside), variables such as voltage across a wire are continuous.  Thus, in digital circuit design, we 
			  view voltages above some threshold as "high" (1), and those below it as "low" (0).  This binary quantization is completely arbitrary, we might just as 
			  well divide the continuous range of voltages into three discrete parts - then we would have a ternary computer.  In practice, however, every computer
			  system has used (at least at this level) binary.  Ternary computers remain an interesting curiosity, see <a href="http://tunguska.sourceforge.net/about.html">
				Tunguska</a> for a ternary computer emulator.</p>
			<p>Data storage, then, in most computers, is just a sequence of bits.  Mathematically speaking, computers store data as a sequence of <i>symbols</i> drawn
			  from the set of possible symbols, called the <i>alphabet</i> (commonly denoted $\Sigma$).  A binary computer is simply the case where $\Sigma = \{1,0\}$
			  .  Note that "1" and "0" are completely arbitrary here; all N-symbol alphabets are completely isomorphic.  We could just as easily have $\Sigma = 
			  \{😃, 😞\}$ or whatever.  Any data we store is simply a way we have of interpreting a sequence of symbols.  Thus, to say "to a computer
			  everything is a number" is (in my opinion) not really accurate, because even the concept of a number is too high-level.  All that exists is a sequence of high
			  and low voltages, or a sequence of symbols - depending if you are an engineer or a mathematician.  All the computer does is manipulate bits (data), and
			  any meaning (information) we attribute to the process is a result of interpretations that exist solely in our minds (and their extensions, our programs).
			  
			  <div id="integers">
				<h4>Representing integers</h4>
				<p>One of the most fundamental things we might wish to represent with a computer are integers.  Since our computers work with discrete symbols,
				  and the integers are discrete objects, the representation is rather natural, though not without its little quirks.  Contrast this with the
				  the reals, where any attempt at representation is doomed to fail - no matter what alphabet or encoding you use, no fixed-length string can hold
				  enough information to represent any real number.  We get by with floating-point (the common standard is <a href="https://en.wikipedia.org/wiki/IEEE_floating_point">IEEE 754</a>), but I consider specific details of floating-point representation outside the scope of this introduction.  (although you should read 
				  <a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>)</p>
				<div id="bcd">
				  <h5>Binary coded decimal</h5>
				  <p>Probably one of the earliest widespread encodings for integers was binary-coded-decimal, or BCD.  In a BCD representation, each base-10 digit of
					a number is encoded seprately, in binary.  That is, </p>
					<table id="bcd_table" style="margin-left: 40px;" border="1"></table>
				  <p>Seems reasonable enough so far.  But in BCD, <i>each decimal digit is encoded individually</i> (hence the name) - for example, the number <code>129</code>
					might be encoded as <code>0001 0010 1001</code>.  Demo: type in a number, see its BCD encoding. (does not work with negatives; BCD traditionally uses
				  a sign digit at the beginning to denote...well, sign)</p>
				  <div>
					<input type="text" value="129" id="bcddemoinput" oninput = "redrawBCDDemo()" />
					<table id="bcddemotable" style="margin-left: inherit; text-align: center;" border="1">
					  <tr border="0"><td>1</td><td>2</td><td>9</td></tr>
					  <tr><td>0001</td><td>0010</td><td>1001</td></tr>
					</table>
				  </div>
					
				  <p>So why use such a seemingly-stupid and baroque system to represent numbers?  Why not just count in binary?  Well, BCD has two main advantages
				  over regular binary place-value encodings.  First, rounding in BCD works just like rounding in decimal: in a regular binary system, numbers such as 0.2
				  cannot be represented exactly (much as 1/3 cannot be represented exactly in a fixed number of decimal digits).  Second, it is very easy to convert
				  BCD to human-readable form, especially in hardware.  A converter that takes the 4 bits of a BCD digit and drives a 7-segment display is just a
				  truth table, and thus can be implemented in hardware.</p>
				  <center>
					From the <a href="http://macao.communications.museum/eng/Exhibition/SecondFloor/MoreInfo/Displays.html">Macao Communications Museum</a>:
					(refers to the individual bits in the BCD representation of a digit as D,C,B,A)
					<br>
					BCD to 7-segment decoder diagram (omits implementation):
					<br>
					<img src="http://macao.communications.museum/images/exhibits/2_18_6_3_eng.png">
					<br>
					<br>
					BCD to 7-segment decoder truth table:
					<br>
					<img src="http://macao.communications.museum/images/exhibits/2_18_6_2_eng.png">
				  </center>
				  <p>
					This is in contrast to a regular binary place-value system, where much more complex hardware would be required to present numbers in decimal format.
					Also, BCD wastes a lot of space compared to a regular binary system.  For some early computers (especially IBM's), the advantages were judged to 
					outweigh the disadvantages, and of course once you have designed a system to use BCD, it is very difficult to change it.  Most designers, however,
					exhibited slightly more sanity, and now (as then) BCD is mostly a curiosity.
				  </p>
				</div>
				<div id="onescomptwoscomp">
				  <h5>One's Complement & Two's Complement</h5>
				  <p>
					Now we'll talk about some systems of representing integers that actually make sense.  What do to with positive numbers is pretty obvious, just
					use a regular binary system with place-value just like how we write decimal.  That is, the number <code>129</code> would be represented as 
					<code>10000001</code> - 1 in 128's place, and 1 in the 1's place for 128 + 1 = 129.  The trick is in representing negative numbers.  In a
					one's complement system, a negative number is represented as being subtracted from the highest representable number.  However, in a two's
					complement system, negative numbers are "subtracted from" the highest representable number <i>plus one</i> - or $2^n$ (where n is the number
					of bits), hence the name.
				  </p>
				  <p>Some examples will make this clearer.  Suppose you have 8 bits, and you want to represent -17.  In a one's complement system, additive negation
					(subtraction from 0) is the same as a bitwise negation, so you take the representation of +17, 10001, and negate it to get 01110.  Now consider
					what happens when you negate 0.  Mathematically, -0 is 0, but in a one's complement system, -0 is the number with all bits set ($2^n -1$).  Of course, 
					using negative zero instead of positive zero won't affect the outcome of any calculations, but the fact that it exists is one of the disadvantages
					of one's complement systems.</p>
				  <p>Essentially all systems today use two's complement, where the number with all bits set is -1 and not -0.  This shifting by one while representing
					negative numbers is the only difference between the two systems.  Two's complement vs. one's complement is a very low-level decision made in the course
					of designing a system - the major differences are observed in designing hardware to do math with binary numbers (an ALU - arithmetic logic unit).
					The design of & distinctions between these circuits is outside the scope of this document, though if you want to learn more about things like that
					(and pretty much everything else that makes up a computer), I highly reccommend <a href="http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319"><i>Code</i> by Charles Petzold</a>.
				</div>				  
			  </div>
			  
			  <div id="text">
				<h4>Representing text</h4>
				<p>In addition to integers, another common basic data type that we often manipulate with computers is text.  Much like integers, text is fundamentally
				  discrete, so we can essentially think of a character encoding as a mapping between characters and numbers.  However, this is a leaky abstraction -
				  characters are not necessarily represented in the same was as integers - and while a system usually has only one encoding for numbers, in the age 
				  of the Internet, it is essential to be able to render text encoded with all sorts of disparate schemes.</p>
				<div id="ebcdic">
				  <h5>EBCDIC</h5>
				  <p>EBCDIC is one of the oldest computer character sets in existence.  The acronym stands for "Extended Binary Coded Decimal Interchange Code", and it was (is)
					primarily used (much like its namesake BCD) on IBM mainframes and suchlike.  However, it has persisted long after most system designers were cured
					of the minor insanity which caused them to design systems based on BCD.  Therefore, you might find yourself using the EBCDIC character set on a system
					that uses regular two's-complement to represent numbers.  This has the interesting consequence that 'a'+1 $\ne$ 'b', for example, and you cannot write
					an <code>isLowerCase</code> function as a simple range check.  Much like BCD, EBCDIC is today mostly a curiosity - a story told to young
					programmers to scare them into using the library versions of functions like <code>isLowerCase</code>, which are at least more likely to run on such
					systems strange enough to use EBCDIC.

				</div>
				<div id="ascii">
				  <h5>ASCII</h5>
				  <p>Originally designed for teleprinters, the American Standard Code for Information Exchange was the first widespread, sane character encoding for
					computers.  ASCII is a seven-bit code (though it is commonly stored in eight-bit bytes), that has unique definitions for all uppercase and lowercase
					characters, control characters, and a familiar set of symbols.  To someone who has worked exclusively with ASCII and its derivatives, these choices
					might seem obvious, but at the time they were not.  Before ASCII, the most common teleprinter code was the <a href="https://en.wikipedia.org/wiki/Baudot_code">Baudot code</a>.  The Baudot system is a five-bit code that makes use of shifts to encode more than $2^5$ characters.  It is also non-contiguous like EBCDIC; when interpreted
					  as binary numbers, 'A'+1 $\ne$ 'B'.  When compared to Baudot, ASCII is much easier to work with on a computer - the fact that you have probably never
					had any huge character encoding headaches when dealing with ASCII text is a testament to its engineering.  </p>
				</div>
				<div id="unicode">
				  <h5>Unicode</h5>
				  <p>ASCII is great if you speak English or another language that uses the Roman alphabet, but if you use the Greek or Cyrillic alphabets, or if (God forbid)
					you use an ideographic writing system, you're SOL.  After the IBM PC was released (and cloned all around the world), this became a problem.  So, clone 
					makers in each country would often use the 'extra bit' that falls out of using 8-bit bytes to encode 7-bit ASCII characters, and invent their own
					proprietary extensions to ASCII.  The version of DOS that shipped with the IBM PC used these extra 128 characters to encode all sorts of things from 
					box-drawing characters (allowing you to draw primitive graphical windows in text mode) to smiley faces and playing-card suits.  In other countries,
					these bytes encoded the alphabet of whatever language was spoken in whatever country the computer was being sold.  It should be apparent that this system
					of many conflicting (even for the same language!) encodings was pretty much a huge mess.  With the internet, it became important to standardize this mess,
					so the ISO standardized a bunch of different widespread encodings into <a href="https://en.wikipedia.org/wiki/ISO_8859">ISO 8859</a>.  This basically
					had the function of keeping these kludgy messes of character encodings in use, and generally just sucked.</p>
				  <p>Even though it sucked horribly, ISO 8859 was the <i>de facto</i> Internet standard encoding for a while.  As long as most computers used Windows,
					most people with alphabetical writing systems weren't being actively excluded from the Internet party.  However, other writing systems, specifically
					Japanese (and to a lesser extent Korean) were pretty much the biggest mess around.  Various different encodings fought for dominance, and a few
					were standardized by Japanese Industrial Standards Committee.  Of these, probably the most widespread is an encoding called Shift-JIS, which is
					rather infamous because it "has the unfortunate property that it often breaks any parser ... that is not specifically designed to handle it."<a href="https://en.wikipedia.org/wiki/Japanese_language_and_computers"><sup>[cite]</sup></a> I'm sure that you don't care about any of this, but the main point of this paragraph was to illustrate just what a complete clusterfuck character encoding(s) can
					devolve into if not handled with extreme care.  In other words, it was (is) a worldwide problem just waiting for a solution...  </p>  
				  <p>
					Enter Unicode!  Unicode is a character encoding system designed to handle every human language - an ambitious task, to be sure.  Technically, 
					what the Unicode standard is a mapping of numbers (<i>code points</i> in Unicode parlance) to characters, divided up into <i>code pages</i>.
					Unicode has everything from Cuneiform to Chinese, and is considered (at least by me) as a major victory of international standardization.
					However, even if everyone agrees to use the Unicode code points, there are still compatability problems, because different implementations
					of Unicode can map code points to bit patterns in different ways.  Java uses a system called UTF-16 internally, whever every character is 
					at least 16 bits (hence the difference between <code>char</code>and <code>byte</code> in Java).  I say "at least", because Unicode has over
					$2^{16}$ code points, and UTF-16 represents those with special escape codes that tell the parser to consider the next byte as part of this
					character.  Contrast this to the obsolete encoding UCS-16, which refused to represent code points beyond $2^{16}$.  There is also an encoding
					called UTF-8, where characters take at least 8 bits (and up to 4).  Every Unicode encoding other than UTF-8 has the rather large disadvantage
					of being completely stupid in pretty much every way.  Because it is an 8-bit format, UTF-8 data looks just like ASCII, so long as you
					don't stray outside of the Latin alphabet. However, unlike ASCII, it is perfectly standardized and can represent every Unicode code point,
					so if your software uses UTF-8, users should be able to input text just as well in Chinese and Cyrillic.</p>
				  <p>Seriously, if there is one thing that you take away from this section on data & data representation, <i>please</i> let it be to use UTF-8
					in any systems you design in later life.  Java made the wrongheaded decision to use UTF-16 for various stupid reasons, so the ship has
					pretty much sailed there, but in every single program I have ever written that has had to deal with multilingual text properly, the 
					<i>only</i> way to get it to work has been to use UTF-8 exclusively for both input and output.  By imploring you to use UTF-8, I wish only 
					to save you, and future users of any systems you may design, from the horrible pain of dealing with any other character encoding than Unicode,
					and any other representation for it	than UTF-8.</p>
				  
				</div>
			  </div>
			  
			  <div id="code">
				<h4>Representing code</h4>
			  </div>
		  </div>
		</div>
		

		<div id="code-program-exec">
		  <h2>Code & Program Execution</h2>
		  <div id="cpus-isas">
			<h3>CPUs & ISAs</h3>
			<p>What a surprise - more three-letter acronyms.  Let's expand them before we do anything else: CPU = Central Processing Unit;
			  ISA = Instruction Set Architecture.  That said, these are two very important and closely-related concepts.  The term CPU refers to the piece of
			  hardware which <i>implements</i> an ISA.  An ISA is, to give a rather formal definition, a mapping from opcodes to actions performed by the CPU.
			  Let's relate both of these terms to another, the all-encompassing <i>machine</i>, via philosophical allegory: the machine (person) needs a CPU
			  (body) to exist within and interact with the Real World, but its ISA (soul) is in many ways conceptually closer to its "identity".  </p>
			<p>For any architecure, extensive documentation of the ISA is absolutely essential - otherwise how would anyone know how to program your machine?
			  Intel makes this documentation publicly available, from
			  <a href="http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html">Intel's web site</a> you can download
			  a complete reference for the x86 and x86-64 architectures, containing every single opcode sorted alphabetically by assembly-language mnemonic.
			</p>
			<p>The person-body-soul analogy given above is a classic example of explaining a CS concept via metaphor - useful at first, but harmful if taken
			  too literally.  For example, in the real world all people we encounter have both body and soul (unless you believe in ghosts).  With CPUs & ISAs,
			  however, this restriction need not apply.  While a CPU that didn't implement an ISA wouldn't be much more than a silicon paperweight, an ISA
			  with no hardware CPU implementing it is quite a usual thing.  Since such ISAs are not designed with hardware in mind, they can make radically 
			  different design choices than "real" ones like x86 and ARM.  A good example is the Java Virtual Machine - it is a stack machine (instead of
			  using registers as the 'immediate working area', all operands are stored on a stack) with some very specialized opcodes, including 
			  <code><a href="http://cs.au.dk/~mis/dOvs/jvmspec/ref--35.html">invokevirtual</a></code> which does all the legwork of virtual-method dispatch
			  in one 'instruction'.  The Java SDK ships with a disassembler for JVM bytecode, <code>javap</code>.  To see what JVM bytecode-assembly looks like,
			  run <br/><code>$ javap -c Whatever.java</code><br/> (-c tells it to include the code in the output, as opposed to just method & class declarations).
			</p>
			<p>One more quick point - although two processors may expose the same ISA to users, their internal implementations need not be at all similar.
			  The guts of an AMD chip implement the x86 ISA in a completely different way than Intel chips do.
			  Now we'll move on to some hotspots where ISAs show a lot of variation.</p>
		  </div>
		  
		  <div id="accessing-data">
			<h3>Accessing Data</h3>
			<p>Since the essential purpose of having a computer is to operate on data, ways to access it are a pretty essential part of any ISA.  We can store
			  a limited amount of data in registers (exactly how limited an amount is an ISA design choice), but after we run out of room there we'll have to
			  somehow work with stuff in memory, and different ISAs do this differently.</p>
			<div id="registers">
			  <h4>Registers</h4>
			  <p>Registers are most basic way to work with data.  Almost all CPU operations expect their inputs exclusively in registers, so everything that 
				we work with on a CPU is probably going to have to wind up in a register sooner or later.  Different ISAs specify different register sizes and
				assembly-language mnemonics.  On most sane N-bit processor architectures, you have maybe 16 or 32 N-bit registers named <code>r0</code> to 
				<code>r[N-1]</code>.  On 32-bit x86, you've got a bunch of 32-bit registers called things like <code>eax</code>, <code>edi</code>, and
				<code>ebp</code>.  These names actually do make sense - skip to the next paragraph if you don't care why.  The first x86 chip, the 8086,
				had a bunch of 16-bit registers including 4 "general purpose" registers: <code>ax</code>, <code>bx</code>, <code>cx</code>, and <code>dx</code>.
				The X was because each half of every register could be addressed separately - <code>ah</code> for the high byte of <code>ax</code>, <code>al</code>
				for the low byte.  When Intel moved to 32-bit, you could still use the old register names, but you meant the 16-bit registers.  The whole 32-bits of
				the registers were denoted by an 'e' for 'extended'.  Hence, <code>eax</code>.  In addition to the general purpose registers, the 8086 had some other
				registers that, either by convention or instruction set mandate, were 'special-purpose'.  One of these was <code>di</code>, the 'destination index'.
				Certain instructions that copy entire strings of memory only work with <code>di</code> and its counterpart <code>si</code> (source index).  And
				finally, <code>bp</code> stands for Base Pointer - it holds the 'base' address of the stack (the start of the region where local variables live).
			  </p>
			</div>
			<div id="buses-addressing">
			  <h3>Memory buses & addressing</h3>
			  <p>Registers are pretty boring - once you've decided on a register machine architecture (the standard in hardware since pretty much forever), you can
				tune the size of the registers and how many there are - that's about it.  Of course, there are other interesting things you can do like 
				<a href="https://en.wikipedia.org/wiki/Register_window">register windows</a>, but they're outside the scope of this document (though the hope is that
				by the end you'll be able to understand the design tradeoffs motivating such things, and the implications of adopting them).  Addressing memory, however,
				is slightly more interesting.  Data travels to and from memory on the memory bus - which used to be a part of the generic system bus but is now (for
				performance reasons) a direct link between the processor and the memory modules.  Design of the memory bus involves deciding what the smallest addressable
				unit of memory is (on most computers, a byte), and how you uniquely specify each location in memory.  This is actually what is meant by an N-bit system -
				a system that can address (uniquely specify) 2^N locations in memory.  More technically, it means the 'width of the address bus' - how many bits are being
				used on the physical address bus to specify an address - but in these modern times we are spared the subtlety; they are nearly always the same.  Note that
				an N-bit machine having N-bit registers is not a necessity - you could have a 24-bit machine with 32-bit registers, for example.  
			  </p>
			</div>
			<div id="harvard-von-neumann">
			  <h3>Harvard & von Neumann architectures</h3>
			  <p> One of the most basic distinctions between computer systems resides in the way they treat memory.  There is more or less a dichotomy, although there are certainly hybrid approaches also.  On a von Neumann machine, code and data reside in the same address space.  This may sound obvious because almost all modern
			  machine behave this way, but it was not always so.  With a Harvard architecture, code and data occupy different address spaces.  At first glance, it may seem
			  that a Harvard architecture needlessly restricts the programmer, but on a closer look this is not a completely bad thing.  Almost all security exploits of software
			  involve tricking a von Neumann machine into executing some code that you have placed in memory by feeding a program malformed data.  The most common of these is
			  a buffer overflow, where you give a program input that is longer than the buffer it has allocated for you, causing it to write the excess of your input into
			  other areas of memory.  The hope is that one of these locations will have previously been home to executable code, and you will be able to cause the program to
			  jump it, hoping to find its own code, but instead finding your carefully-crafted <a href="https://en.wikipedia.org/wiki/Shellcode">shellcode</a>.  Since you
			  might not be sure where exactly the program will jump, you might do something like a <a href="https://en.wikipedia.org/wiki/NOP_slide">NOP slide</a> in hopes
			  of increasing the chances of your shellcode being executed.  As you can see, this attack fundamentally relies on code and data sharing the same address space -
				if there were an insurmountable Berlin (*cough* Cambridge) Wall between them as in a Harvard architecture, you could never make this mistake.</p>
			  <p>Harvard architectures can also be faster then von Neumann machines at certain things (if code & data are two different address spaces, you can read/write both
				at the same time, for example), and the hardware can be simpler to implement.  But, in general, modern computer architects have considered giving up all the pros
				of the von Neumann architecture too steep a price to pay - no von Neumann machines, no JITs, for example.  So, generally, Harvard architectures are only found in
				smaller special purpose chips such as those used in DSP (digital signal processing).  That's not to say Harvard architectures haven't influenced modern CPU design
				- almost all modern chips do implement some sort of tagging system in hopes of recapturing some of the security benefits of Harvard machines, and most have
				separate caches for instructions and data.</p>
			</div>
			<div id="addressing-modes">
			  <h3>Addressing modes</h3>
			  <p>Now that we've defined our terms and suchlike, we can look at the real heart of the issue so far as ISA design is concerned - addressing modes.  The addressing
				modes of a processor are the different ways you can specify places in memory to those instructions that can act on memory locations.  In the simplest possible
				scheme, the only such instruction is a move instruction, and the only addressing mode is 'absolute' - you have to know the exact memory address beforehand (as 
				in, at assemble-time).  However, this isn't very useful.  Want to index an array?  Too bad.  Now we have to cave and add (at the very least) a 'register-indirect'
				mode, where we can specify a memory address as being the contents of some register.  In a perfect (or RISC - see Part IV) world, this is all we need.  However,
				there are a bunch of other patterns in memory addressing that we could probably speed up if we added more hardware and more ISA complexity.  For example,
				we might want C code to more easily access its local variables.  On x86, each local variable is usually stored at <code>ebp - K</code>, where K is the sum
				of the sizes of all previously-declared local variables in the stack frame (in other words, a unique constant per local variable).  So we'll add another address 
				mode that handles the register+constant case.  Oh, but you know what?  Indexing arrays is pretty common, so we'll probably want to do that.  For that we'll
				need another addressing mode that lets us constant+index_register.  And so on, and so forth.  On x86, it ended up being the case that they made an entire
				unit on the chip dedicated to calculating addresses - a sort of specialized ALU linked directly to the memory bus. </p>
			  <p>This enabled one of my favorite low-level hacks of all time - the <code>lea</code> trick.  Since Intel put this whole adress-calculation-unit on the die, they
				also gave the programmers an instruction that used it to calculate the 'effective address' and store it in a register - Load Effective Address, or
				<code>lea</code>.  Since this specialized ALU could do fewer things than the 'real' ALU, it was often slightly faster for those operations it could do - 
				calculations of the form base + K*index, generally, though this is nowhere near the maximum complexity achieavable.  
				Because of this, and because of pipelining (see Part IV), it was (is?) considered a clever performance trick to use the <code>lea</code> instruction to do
				ordinary arithmetic for you when you could.</p>				
			</div>
		  </div>
		</div>		
		<div id="building-system">
		  <h2>Putting together the system</h2>
		  <div id="osconcepts">
			<h3>OS concepts</h3>
			<p>...</p>
		  </div>
		 
		  <div id="introc">
			<h3>Introduction to C (for Java programmers)</h3>
			<div id="cessence">
			  <h4>The essence of C</h4>
			  <p>Before embarking on a quest to learn C, I think it might be useful to understand the motivation for its design, and some of the core design philosophies
				it chose as a result.  C is a language designed for writing operating systems.  Some people like to say things like "C is a language for systems programming",
				which they intend to mean OSes and low-level userspace tools (maybe webservers and stuff).  That might be true, and certainly a lot of non-OS system-level
				software is written in C, but the project for which C was created was to write the world's first OS in a high-level language (namely, Unix).  Because of this,
				C chooses to be a fairly thin layer over assembly.  Compiler optimizations notwithstanding, it should be fairly easy to look at a piece of C code and see
				a sketch of what assembly you'll get.  C also generally assumes you know what you're doing - you get enough rope to hang yourself and then some, and nobody's
				going to catch you when you fall.  Lastly, since C is a language 'for' OS development, it shouldn't depend on anything - the C runtime should be nothing more
				than a small number of functions you can call if you want, as opposed to doing things like dynamic type/bounds-checking and garbage collection (note that these
				things would violate the transparency goal by making it difficult to know what assembly you'll get from the compiler).  To sum up, C is a great language
				for being close to the <i>machine</i> - if that's what you need, C will serve you well.  If not, you won't have a good time. </p>
			</div>
			<div id="include-and-preprocessor">
			  <h4><code>#include</code> and the preprocessor</h4>
			  <p>Since in C (compared to Java) most of the control-flow concepts are the same, and most features (OOP, garbage collection) are simply absent, I'm going to
				go over those areas were Java and C solve the same problem in different ways, or C solves a problem that Java sweeps under the rug. 
			  <p>We'll start off with
				code re-use.  In Java, our .java source files both <i>declare</i> and <i>define</i> things, and when we want to use code we (or someone else) has already
				written, we "import" a "class".  This is much too fancy for C.  In C, you have two types of files: <code>.h</code> (header) files which <i>declare</i>
				things, and <code>.c</code> (code) file which <i>define</i> them.  When you declare something, you're simply notifying the compiler that you will somewhere
				later define it, and in return the compiler lets you use it in your code.  The way you 'import' the definitions in .h files is with the <code>#include</code>
				directive.
			  </p>
			  <p>In Java, <code>import</code> is considered a statement just like <code>break</code> or <code>return</code> - it's parsed and understood by the same bit of
				code that compiles the rest of your source.  In C, <code>#include</code> is NOT a statement, and it's never seen by the compiler proper.  Instead, there's
				another program, the C preprocessor, which sees your source and modifies it before the compiler sees it.  All lines beginning with a # sign are for the 
				preprocessor.  <code>#include "file.h"</code> tells the preprocessor to read the contents of file.h and paste them into the file just as they are - very
				low-tech.  When specifying the name of the header to include, you can use angle brackets (example: <code>#include &lt;stdio.h&gt;</code>) or quotes (as before).
				Angle brackets tell the preprocessor to look in the standard library for the header, quotes tell it to look in the current directory.  Note that the name in
				quotes of angle-brackets is actually a filename, so you can have a hierarchy of headers and do things likie <code>#include "algorithms/sorting/heapsort.h"</code>.
				Also note that there might or might not be a .c file for every .h - it might be an already-compiled object file (extensions .a, .o, .so) - but you use
				it the same way.
			  </p>
			  <p>There are other preprocessor directives besides <code>#include</code>, I recommend reading <a href="https://en.wikipedia.org/wiki/C_preprocessor">Wikipedia</a>
				if you feel like learning more about the preprocessor.  In case you were wondering, the C preprocessor is in fact Turing-complete (within the limitations of
				compilers to let you include yourself as a means of recursion / looping), see <a href="http://www.ioccc.org/years-spoiler.html">the IOCCC</a> (International
				Obfuscated C Code Contest) for an implementation of the Towers of Hanoi in the preprocessor (search for "hanoi").</p>
			</div>
			
			<div id="zomg-pointers">
			  <h4>Pointers</h4>
			  <p>Now we arrive at the topic I was referring to when I mentioned "C solves a problem that Java sweeps under the rug" - pointers.  We know about assembly,
				and we know about addressing modes, so it should be old news that one of the things you can store in a register is a memory location, and that you can
				later do things like write into that location, read from it, etc.  A pointer is just a variable that stores a memory address of something.  The syntax
				for declaring a pointer looks like: <br>
				<code>T *myPointer;</code>
				<br>
				where T is any type.  </p>
			  
			  <p>Once you have a pointer (also called a reference), you can manipulate either it itself (the reference, the address, etc), or the thing at the address
				it points to (the referent).  Manipulating <i>the reference itself</i> uses the variable name unadorned, while manipulating the <i>referent</i> you must
				use a the * operator to <i>dereference</i> the pointer.  The inverse of * (pronounced 'dereference' or 'star') is & (addressof).  & takes 
				<i>any variable</i> (including a pointer) and gives you a pointer to it.  This is confusing - let's look at some example code:</p>

			  <pre><code>
					int a = 4, b = 1024;
					int *p = &a			//"p equals addressof a" - p now points to a
					*p = 5;				//we are changing p's referent, AKA a - after this, if we read a, it will be 5
					p = &b;				//we are changing p - it now refers to b
					*p = 7;				//now (b == 7) is true
					p = a;				//WRONG!  should give a compile warning - p and a are both numbers, but this assignment is almost certainly
					//a bad idea, unless we want a pointer to memory address 4 (which will probably crash the program on read/write)
			  </code></pre>
			  
			  <p>Like I said before, * and & are inverses, so applying them both is an identity:
				<code>someVar == *(&someVar)</code> is always true
				(note also from this example that * can be applied to any expression with pointer type)
			  </p>
			  
				
			  <p>A quick aside: C has things called lvalues and rvalues that you might hear about in compiler error messages.  An lvalue is something
				that can appear on the left side of an equals sign (can be <i>assigned to</i>), an rvalue can only appear on the right side of an equals sign (can only be
				<i>assigned from</i>).  & acts on lvalues ONLY - something like <code>int *p = &7</code> doesn't make sense any more than <code>7 = x</code>
			  </p>
			  
			  <p>With a basic understanding of pointers, we are now equipped to talk about C arrays.  An array in C is just a block of memory with some length.  (it's
				your job to keep track of the length, you can't ask an array it's length - it's just a block of memory!) The actual
				'array variable' is just a pointer to the 0th element, so array subscripting is just pointer arithmetic:<br>
				<code>array[idx] == *(array + idx)</code><br>
				There is a very important and subtle point (heh) here, don't miss it: The 'unit' in pointer arithmetic is <i>always</i> the size of the type of the pointer. 
				(C gives you the <code>sizeof</code> operator to find out the size of things in bytes: <code>sizeof(variable)</code> or <code>sizeof(type)</code> evaluates to
				the size (in bytes) of the type).
				If you said that <code>ptr+1</code> was always the address of ptr plus one byte, then you would have to do a multiplication by <code>sizeof(*array)</code> (note
				that since the variable 'array' is a pointer, <code>sizeof(array)</code> would give us the size of a pointer, which is the same no matter what it points to)
				in the above array-subscripting example.  C assumes that would be annoying, so it effectively is doing it for you.  The way to get the address of a pointer 
				'plus N bytes' is to cast the pointer to <code>char *</code> before doing the pointer arithmetic.  Example: to get a pointer 7 bytes into a block of memory
				pointed at by 'myPtr':<code>char *sevenBytesPast = ((char *)myPtr) + 7</code>
			  </p>
			  
			  <p>Strings in C are just arrays of type <code>char</code>, which is to say that they are just char pointers.  By convention (your program will likely crash
				if you don't obey this convention) the end of a C-string is marked by a byte with value 0 - called a 'null terminator'</p>
			  
			  <p>The array-pointer duality has some interesting results.  One of them is that it is very easy to pass a function the 'last N' cells of an array - just call
				<code>function(array + (N - S))</code> (where S is the size of the array).  So for example here's a binary search function (it's probably buggy but it illustrates
				the basic idea):
			  </p>
			  <!--			  <pre><code>
							  int binsearch(int[] ar, int key, int size) {	//search the first 'size' cells in 'ar' for 'key'
							  if (size == 0)
							  return -1;
							  int mid = size / 2;
							  if (ar[mid]  > key)
							  return binsearch(ar, key, mid - 1);
							  if (ar[mid] < key)
							  return binsearch(ar + mid + 1, key, mid);
							  else return ar[mid];
							  }
							  </pre></code>			  !-->
			</div>
			
			<div id="structunion">
			  <h4><code>struct</code> & <code>union</code></h4>
			  <p>It is often useful to group a number of variables into one package that can be manipulated as a unit, we might for example want to define 2D points by
				grouping two ints, x and y.  The tool C gives you to do that is a <code>struct</code> (structure).  You can think of a struct as an object with no methods,
				just public data members.  Example:
				<pre><code>
					struct point {
						int x, y;
					}

					//now the name of the type is 'struct point' - we would write
					struct point reflect(struct point p) {
						struct point np;
						np.x = p.y;
						np.y = p.x;
						return np;
					}
					
					//if we don't like that, we can use typedef:
					typedef struct {
						int x, y;
					} point;
					
					//now we can use 'point' just like any other type.
				</code></pre>
				</p>
			  <p>However, there is also another case - where one unit can have multiple possible values, but only one at a time.  To do this, C gives us unions - 
				whereas sizeof(struct S) is the sum of its member variable sizes, sizeof(union U) is the <i>maximum</i> of its member variable sizes.  Writing to a
				field in a union leaves all other fields in an undefined state, because their space is being used to store the value you just wrote.  Since only the last-written
				member of a union is ever valid, unions are often nested inside structs which contain the union and a 'tag' identifying what type the union currently represents.
				I wouldn't go so far as to call unions esoteric, but you might not 'see' situations where they are a good idea if you come from Java which has no analogous
				concept, and so you might not use them a lot. </p>
			</div>
			
			<div id="mallocfree">
			  <h4>Dynamic memory allocation: <code>malloc</code> & <code>free</code></h4>
			  <p>Not all quantities are known at compile-time.  If we ask for input from the user, we could just declare a really big static array (fixed-size) and hope they
				don't go over (or we can't store it, potentially with a buffer overflow vulnerability).  Or we could allocate memory dynamically, and ask the system to 
				provide us with more memory at runtime.  In Java, you do this with the <code>new</code> keyword - you just create a new object and Java worries about where
				the memory comes from and where it goes when you are done.  Not so in C.  In C dynamic memory allocation is done by a regular old function that lives in the
				header &lt;stdlib.h&gt;, called <code>malloc</code>.  The type declaration for malloc looks like: <code>void *malloc(size_t size)</code>.  size_t is an
				unsigned (positive only) integer type used to represent, well, size.  <code>void *</code>, a "void pointer", represents a pointer to a block of memory that
				holds values of some indeterminate type.  Since C has no generics or templates, this is all malloc can give you - you have to cast it to what you want yourself.
				To allocate an array of ints with some variable size:
				<br>Java: <code>int[] dynArray = new int[size];</code>
				<br>C:<code>int *dynArray = (int *)malloc(size * sizeof(int)); </code>
				<br>Note that the C version actually uses pointers instead of arrays.  They're the same thing (sort of) as we discussed, and it's easier and customary to just
				use pointers with malloc, even if you are allocating things you might call 'arrays'</p>
			  <p>Memory management in C has another dimension: freeing memory when you are done!  In Java the runtime has a garbage collector that finds memory that you are not using for anything anymore and reclaims it so the same space can be eaten by future calls to <code>new</code>.  In C you have to do this yourself, by calling another regular old
				function, <code>free</code>.  free just takes one argument, the same pointer you got from malloc (since it is declared as void* in free's argument list, you
				needn't cast it back to void* yourself).  If you free something and then try to use it - you get undefined behavior (often a crash).  If you free something and 
				then try to free it again - probably a crash.  If you never free anything you'll keep going until eventually you will run out of memory, at which point 
				<code>malloc</code> will return <code>NULL</code> (an edge case worth checking for).  This (not freeing memory) is called a memory leak.  Ever used a program
				or game than ran smoothly the first few minutes and then got real, real slow?  Probably a memory leak.  </p>
			</div>
			
		  </div>		  
		</div>
      </section>
    </div>

    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
		<p>Math rendered with <a href="http://www.mathjax.org/">MathJax</a></p>
      </footer>
    </div>
  </body>
</html>

